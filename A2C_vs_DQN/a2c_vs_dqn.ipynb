{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601df869",
   "metadata": {},
   "source": [
    "# A2C versus DQN\n",
    "- comparison of A2C to DQN on simple control tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61617a5",
   "metadata": {},
   "source": [
    "### problem statement\n",
    "\n",
    "Goal of RL is to maximize expected value $\\mathbb{E}_\\pi[v(s)]$ \n",
    "\n",
    "Given a paramtrized policy $\\pi_\\theta$, the criterion is: $J(\\theta) = \\mathbb{E}_\\pi[V(s)]$\n",
    "\n",
    "The expectation is with respect to environment and policy. Let $d(s)$ be the probability of occupying state $s$, expand the state value, and you can sample from this expectation:\n",
    "\n",
    "$\\mathbb{E}_\\pi[v(s)] = \\sum_s d(s) \\sum_a \\pi_\\theta(a|s) q(s,a)$  where  $\\pi_\\theta$ is parametrized policy\n",
    "\n",
    "thus, the goal is therefore to find a $ \\theta $ that maximizes $J(\\theta)$:\n",
    "\n",
    "$argmax_\\theta J(\\theta) = argmax_\\theta \\mathbb{E}_\\pi[V(s)] = argmax_\\theta \\sum_s d(s) \\sum_a \\pi_\\theta(a|s) q(s,a) $\n",
    "\n",
    "one way to do this is to do gradient ascent on $J(\\theta)$.\n",
    "\n",
    "\n",
    "### policy gradient\n",
    "\n",
    "importantly, state visitation probability $d(s)$ depends on policy, so the full expression for $ \\nabla_\\theta J(\\theta)$ depends on the gradient of the state distribution $ d(s)$:\n",
    "\n",
    "$\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\sum_s d(s) \\sum_a\\pi_\\theta(s|a) Q(s,a)$\n",
    "\n",
    "fortunately, the policy gradient allows us to get away without having to compute this gradient:\n",
    "\n",
    "$\\nabla_\\theta J(\\theta) \\propto \\sum_s d(s) \\sum_a Q(s,a) \\nabla_\\theta \\pi_\\theta(s|a) $\n",
    "\n",
    "\n",
    "the following trick, where the above is multiplied by 1,\n",
    "allows us to express this approximation as an expectation, from which we can sample\n",
    "\n",
    "\n",
    "$\\nabla_\\theta J(\\theta) \\propto \\sum_s d(s) \\sum_a  \\frac{\\pi_\\theta(s|a)}{\\pi_\\theta(s|a)}Q(s,a) \\nabla_\\theta \\pi_\\theta(s|a) $\n",
    "\n",
    "$ = \\mathbb{E}_\\pi [ \\frac{\\nabla_\\theta \\pi_\\theta(s|a)}{\\pi_\\theta(s|a)}Q(s,a)  ] $\n",
    "\n",
    "$ = \\mathbb{E}_\\pi [ Q(s,a) \\nabla_\\theta \\ln \\pi_\\theta(s|a)   ] $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461bd67",
   "metadata": {},
   "source": [
    "#### REINFORCE:\n",
    "\n",
    "$\\mathbb{E}_\\pi [ Q(s,a) \\nabla_\\theta \\ln \\pi_\\theta(s|a) ] $\n",
    "\n",
    "estimate $Q(s,a)$  using episodic returns $G_t$ \n",
    "\n",
    "$ \\mathbb{E}_\\pi [ G_t \\nabla_\\theta \\ln \\pi_\\theta(s|a)  ]$ \n",
    "\n",
    "can also baseline with advantage function $A(s,a)$ in place of return $G_t$\n",
    "\n",
    "$ A(s,a) = Q(s,a) - V(s)$ \n",
    "\n",
    "when implementing this, $Q(s,a)$ is replaced by return $G_t$, and $V(s)$ is parametrized value approximator\n",
    "\n",
    "#### A2C:\n",
    "uses TD error (computed online) instead of monte carlo estimates (requires full episode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8b52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from utils import *\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch as tr\n",
    "import gym\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d208911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "def experiment(nseeds,neps,kw):\n",
    "  \"\"\"\n",
    "  interactrion logic\n",
    "  a < agent(s,h)\n",
    "  s',r  < env (a)\n",
    "  \n",
    "  \"\"\"\n",
    "  metric = np.zeros((ns,neps))\n",
    "  # loop over seeds\n",
    "  for s in range(ns):\n",
    "    np.random.seed(s)\n",
    "    tr.manual_seed(s)\n",
    "    # setup\n",
    "    agent = kw['agent']\n",
    "    task = Task(max_ep_len=kw['max_ep_len'])\n",
    "    buffer = Buffer('episodic',kw['buff_size'])\n",
    "    for e in range(neps):\n",
    "        ## score on greedy policy\n",
    "        episode = task.play_ep(agent.softmax_policy_fn(1.0))\n",
    "        metric[s,e] = np.sum(unpack_expL(episode)['reward'])\n",
    "        ## train on softmax\n",
    "#         episode = task.play_ep(agent.softmax_policy_fn(0.85))\n",
    "#         buffer.record(episode)\n",
    "#         expLoD = buffer.sample(kw['batch_size'])\n",
    "#         # update\n",
    "#         exp = unpack_expL(expLoD)\n",
    "#         agent.train(exp)\n",
    "  return metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dacf7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# run experiments\n",
    "ns,ni=1,1\n",
    "kwargs = {'buff_size':500,'batch_size':128,'max_ep_len':100,'buff_mode':'episodic'}\n",
    "# REINFORCE\n",
    "kwargs['buff_mode']='episodic'\n",
    "kwargs['agent']=PolValNet()\n",
    "m = experiment(ns,ni,kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8815e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from distant past\n",
    "kwargs['buff_mode']='online'\n",
    "m_online = experiment(ns,ni,kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbcc51",
   "metadata": {},
   "source": [
    "### visualize sum of episode rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2c3ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plt_metric(metric):\n",
    "  # seeds and mean\n",
    "  M = metric.mean(0)\n",
    "  S = metric.std(0)/np.sqrt(ns)\n",
    "  ax = plt.gca()\n",
    "  ax.plot(M,zorder=99)\n",
    "  for smet in metric:\n",
    "    ax.plot(smet,c='k',lw=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad61dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [m_online,m_episodic]:\n",
    "  plt_metric(metric)\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-10,kwargs['max_ep_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54be528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b89e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b459938",
   "metadata": {},
   "source": [
    "#### resources\n",
    "- [mnih et al., 2015](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "- [blog](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)\n",
    "- [lilian weng blog](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#a3c)\n",
    "- [A3C paper](https://arxiv.org/pdf/1602.01783.pdf)\n",
    "- [TF implementation](https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
